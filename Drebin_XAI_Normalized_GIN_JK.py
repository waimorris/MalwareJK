from torch_geometric.data import Data, DataLoader
from torch_geometric.datasets import TUDataset, Planetoid
from torch_geometric.nn import GCNConv, Set2Set, GNNExplainer
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T
import torch
import torch.nn.functional as F
import os
from tqdm import tqdm, trange
import networkx as nx
from torch_geometric.nn import Node2Vec

from sklearn import preprocessing

import pickle
import random

import matplotlib.pyplot as plt

le = preprocessing.LabelEncoder()
le.fit(['benign','malware'])

class DeribnDataset():
    def __init__(self):
        self.graphs = []
        self.labels = []
        self.features = np.empty((0,4), float)
        
    def process(self):
        for subdir, dirs, files in os.walk('/content/malnet-graphs-tiny/'):
          for idx,file in enumerate(files):
              if file in train_sample:
                filepath = subdir + os.sep + file
                label = os.path.split(os.path.split(subdir)[0])[1]

                g = nx.read_edgelist(filepath, create_using=nx.DiGraph())
                print(g)
                g = nx.convert_node_labels_to_integers(g, first_label=0, ordering='default', label_attribute=None)
                feat_data = np.zeros((g.number_of_nodes(), 4))
                pagerank = nx.pagerank(g)
                in_degree = nx.in_degree_centrality(g)
                out_degree = nx.out_degree_centrality(g)
                closeness_centrality = nx.closeness_centrality(g)

                g = from_networkx(g)
                        
                for k, v in pagerank.items():
                    feat_data[k][0] = v

                for k, v in in_degree.items():
                    feat_data[k][1] = v

                for k, v in out_degree.items():
                    feat_data[k][2] = v

                for k, v in closeness_centrality.items():
                    feat_data[k][3] = v

                g.x = torch.from_numpy(feat_data) 
                g.y = le.transform([label])[0]
                self.graphs.append(g)
                self.labels.append(label)
                self.features = np.append(self.features, feat_data,axis=0)
                print(idx)


        self.labels = le.transform(self.labels)
        self.labels = torch.LongTensor(self.labels)
      
    def __getencoder__(self):
        return self.le

    def __getitem__(self, i):
        return self.graphs[i], self.labels[i]

    def __len__(self):
        return len(self.graphs)

with open('train_loader_derbin_pagerank_degree_betewenness_normalized.pkl', 'rb') as file:
     train_data = pickle.load(file)

with open('test_loader_derbin_pagerank_degree_betewenness_normalized.pkl', 'rb') as file:
     test_data = pickle.load(file)

for index,i in enumerate(train_data.graphs):
  if (i.num_nodes) == 0:
    train_data.graphs.pop(index)

for index,i in enumerate(test_data.graphs):
  if (i.num_nodes) == 0:
    test_data.graphs.pop(index)

# for index,graph in enumerate(train_data):
#   train_data[index].y = train_data[index].y[0]

# for index,graph in enumerate(test_data):
#   test_data[index].y = test_data[index].y[0]

train_loader = DataLoader(train_data.graphs, batch_size=32,shuffle=True) 
test_loader = DataLoader(test_data.graphs, batch_size=16)

import torch
import torch.nn.functional as F
from torch.nn import Linear
from torch_geometric.nn import SAGEConv, global_mean_pool, JumpingKnowledge
from torch_geometric.nn import SAGEConv, GlobalAttention

from torch_geometric.nn import (GraphConv, TopKPooling, global_mean_pool,
                                JumpingKnowledge)

import torch
import torch.nn.functional as F
from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN
from torch_geometric.nn import GINConv, global_max_pool, JumpingKnowledge

class GINWithJK(torch.nn.Module):
    def __init__(self, num_layers, hidden):
        super(GINWithJK, self).__init__()
        self.conv1 = GINConv(
            Sequential(
                Linear(4, hidden),
                ReLU()
            ))
        self.convs = torch.nn.ModuleList()
        for i in range(num_layers - 1):
            self.convs.append(
                GINConv(
                    Sequential(
                        Linear(hidden, hidden),
                        ReLU()
                    ), train_eps=False))
        self.jump = JumpingKnowledge('cat')
        self.lin1 = Linear(hidden * 6, hidden)
        self.lin2 = Linear(hidden, 2)

    def reset_parameters(self):
        self.conv1.reset_parameters()
        for conv in self.convs:
            conv.reset_parameters()
        self.jump.reset_parameters()
        self.lin1.reset_parameters()
        self.lin2.reset_parameters()

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        xs = [x]
        for conv in self.convs:
            x = F.relu(conv(x, edge_index))
            xs += [x]
        x = self.jump(xs)
        x = global_max_pool(x, batch) 
        x = F.relu(self.lin1(x))
        x = self.lin2(x)
        return F.log_softmax(x, dim=-1)

model = GINWithJK(6,128).cuda()
print(model)

import torch.nn as nn

weights = torch.tensor([1., 2.5]).cuda()

optimizer = torch.optim.Adam(model.parameters())
criterion = torch.nn.CrossEntropyLoss(weight=weights)

from torch.utils.tensorboard import SummaryWriter

! rm -r runs

tb = SummaryWriter()

# %load_ext tensorboard
# %tensorboard --logdir /content/runs/Dec16_01-39-15_ce3386070523

# from google.colab import drive
# drive.mount('/content/drive')

for epoch in range(1, 5000):
    best_acc = 0
    correct = 0
    test_correct = 0
    for data in train_loader:  # Iterate in batches over the training dataset.
         data = data.to('cuda')
         out = model(data.x.float(), data.edge_index, data.batch).cuda()  # Perform a single forward pass.
         ground_truth = torch.as_tensor(data.y).to('cuda')
         loss = criterion(out, ground_truth)  
         _, predicted = torch.max(out.data, 1)
         correct += (predicted == ground_truth).float().sum()
         loss.backward()  
         optimizer.step()  
         optimizer.zero_grad()  
    training_accuracy = 100 * correct / len(train_data.graphs)

    print(epoch)
    print("Training Accuracy = {}".format(training_accuracy))
